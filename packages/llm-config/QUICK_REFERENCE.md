# LLM Config Quick Reference

## Setup (30 seconds)

```bash
# 1. Copy env template
cp .env.example .env

# 2. Edit .env - add your API key OR setup local model
# For OpenAI: LLM_OPENAI_API_KEY=sk-...
# For Local: ollama pull llama3.1

# 3. Install & build
cd packages/llm-config
pnpm install && pnpm build
```

## Basic Usage

```typescript
import { LLMManager } from '@spynet/llm-config';

const llm = new LLMManager();

// Simple chat (returns string)
const text = await llm.simpleChat('Generate a spy codename');

// Full chat (returns detailed response)
const response = await llm.chat([
  { role: 'system', content: 'You are helpful' },
  { role: 'user', content: 'Hello!' }
]);
```

## Provider Management

```typescript
// Check active provider
llm.getActiveProvider(); // 'openai'

// List available providers
llm.getAvailableProviders(); // ['openai', 'local']

// Switch provider
llm.switchProvider('local');

// Use specific provider
await llm.chatWithProvider('openai', [...]);

// Automatic fallback
const res = await llm.chatWithFallback([...]);
console.log(res.usedProvider); // Which provider succeeded
```

## Environment Variables

```env
# Required: Choose active provider
LLM_ACTIVE_PROVIDER=openai|anthropic|local|custom

# Per provider (replace {PROVIDER} with OPENAI, ANTHROPIC, LOCAL, or CUSTOM)
LLM_{PROVIDER}_ENABLED=true
LLM_{PROVIDER}_API_KEY=your-key
LLM_{PROVIDER}_BASE_URL=https://api.example.com/v1
LLM_{PROVIDER}_MODEL=model-name
LLM_{PROVIDER}_MAX_TOKENS=2000
LLM_{PROVIDER}_TEMPERATURE=0.7
```

## Provider URLs

| Provider | Base URL | Get Key |
|----------|----------|---------|
| OpenAI | `https://api.openai.com/v1` | https://platform.openai.com/api-keys |
| Anthropic | `https://api.anthropic.com/v1` | https://console.anthropic.com |
| Ollama | `http://localhost:11434/v1` | N/A (local) |
| LM Studio | `http://localhost:1234/v1` | N/A (local) |
| Together AI | `https://api.together.xyz/v1` | https://api.together.ai |
| Groq | `https://api.groq.com/openai/v1` | https://console.groq.com |

## Common Patterns

### Cost Optimization
```typescript
// Try local first (free), cloud if needed
llm.switchProvider('local');
const res = await llm.chatWithFallback([...]);
```

### Task Routing
```typescript
// Complex task → powerful model
llm.switchProvider('openai');
const complex = await llm.chat([...]);

// Simple task → fast/cheap model
llm.switchProvider('local');
const simple = await llm.chat([...]);
```

### Development vs Production
```typescript
// .env.development
LLM_ACTIVE_PROVIDER=local

// .env.production
LLM_ACTIVE_PROVIDER=openai
```

## Testing

```typescript
// Test active provider
await llm.testActiveConnection(); // boolean

// Test all providers
const results = await llm.testAllConnections();
for (const [provider, ok] of results) {
  console.log(`${provider}: ${ok ? '✓' : '✗'}`);
}
```

## Chat Options

```typescript
await llm.chat([...], {
  temperature: 0.7,      // 0-2, creativity
  maxTokens: 500,        // response length
  topP: 0.9,             // nucleus sampling
  frequencyPenalty: 0.5, // reduce repetition
  presencePenalty: 0.5,  // encourage topics
  stop: ['END']          // stop sequences
});
```

## Error Handling

```typescript
import {
  LLMError,
  LLMConnectionError,
  LLMAuthenticationError,
  LLMRateLimitError,
  LLMTimeoutError
} from '@spynet/llm-config';

try {
  await llm.chat([...]);
} catch (e) {
  if (e instanceof LLMAuthenticationError) {
    // Invalid API key
  } else if (e instanceof LLMRateLimitError) {
    // Too many requests
  } else if (e instanceof LLMConnectionError) {
    // Network issue
  }
}
```

## Debugging

```typescript
// Get config summary (no secrets)
console.log(llm.getConfigSummary());

// Output:
// {
//   activeProvider: 'openai',
//   enabledProviders: ['openai', 'local'],
//   providers: {
//     openai: { enabled: true, model: 'gpt-4', hasApiKey: true },
//     ...
//   }
// }
```

## Ollama Setup

```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Pull a model
ollama pull llama3.1:8b   # Fast (4.7GB)
ollama pull llama3.1:70b  # Better (40GB)

# Start server (if not auto-started)
ollama serve

# Test
curl http://localhost:11434/v1/models
```

## Full Example

```typescript
import { LLMManager } from '@spynet/llm-config';

async function generateMission() {
  const llm = new LLMManager();

  // Use best available provider
  const response = await llm.chatWithFallback([
    {
      role: 'system',
      content: 'You are a spy mission coordinator.'
    },
    {
      role: 'user',
      content: 'Generate a mission briefing for San Francisco'
    }
  ], {
    temperature: 0.8,
    maxTokens: 300
  });

  console.log(`Generated by: ${response.usedProvider}`);
  console.log(`Tokens: ${response.usage.totalTokens}`);
  console.log(`Mission: ${response.content}`);
}

generateMission();
```

## Tips

- Enable multiple providers for redundancy
- Use `chatWithFallback()` in production
- Route by task: complex → cloud, simple → local
- Local models need `ollama serve` running
- Check `hasApiKey: true` in config summary
- Temperature 0.0-0.3: factual, 0.7-1.0: creative

## Troubleshooting

| Issue | Solution |
|-------|----------|
| Connection refused (local) | Run `ollama serve` |
| Invalid API key | Check key, no extra spaces in .env |
| Model not found | `ollama list`, then `ollama pull model-name` |
| Rate limit | Use `chatWithFallback()`, enable local backup |
| Timeout | Increase `LLM_DEFAULT_TIMEOUT_MS` |

## More Info

- Full docs: `packages/llm-config/README.md`
- Setup guide: `docs/LLM_SETUP.md`
- Examples: `packages/llm-config/examples/`
